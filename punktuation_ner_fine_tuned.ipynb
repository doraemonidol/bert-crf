{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcxV9bgteVTS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2ZcDs9WeYC1"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/gdrive/MyDrive/Colab/punktuation-ner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-Sxoj2-vLk1"
      },
      "outputs": [],
      "source": [
        "!pip install forgebox==0.4.18.5 pytorch_lightning sklearn-model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW2xYPCQsdjq"
      },
      "source": [
        "# Punctuation NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nAfO_Zxsdjr"
      },
      "outputs": [],
      "source": [
        "# Forgebox Imports\n",
        "from forgebox.imports import *\n",
        "from forgebox.category import Category\n",
        "import pytorch_lightning as pl\n",
        "from transformers import AutoTokenizer, BertForTokenClassification, BertModel\n",
        "from transformers import pipeline\n",
        "from typing import List\n",
        "import re\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK0e6KrmvyOv"
      },
      "outputs": [],
      "source": [
        "DATA = r'/content/gdrive/MyDrive/Colab/punktuation-ner/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXq-WPgBZxrH"
      },
      "outputs": [],
      "source": [
        "# In data/couplets/Chinese_couplet_dataset_sample_2K.tsv, the format is as follows:\n",
        "# first,second,label\n",
        "# 明 有 通 人 著 赤 雅                                                                            ,汉 称 孝 子 褒 黄 香,0000000\n",
        "# 栖 霞 山 上 栖 霞 寺                                                                            ,建 业 城 中 建 业 人,0000000\n",
        "# 修 身 如 执 玉                                                                                  ,行 善 胜 遗 金,00000\n",
        "# 闲 云 归 岫 连 峰 暗                                                                            ,飞 瀑 垂 空 漱 石 凉,0000000\n",
        "# 雪 寂 春 薄 ， 谁 怜 取 寒 香 一 抹                                                             ,夜 深 人 静 ， 伊 独 饮 醉 苦 千 般,000010000000\n",
        "\n",
        "# Load the data and use train_test_split to split the data into training and validation sets, save the data to the data folder\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(f\"{DATA}/Chinese_couplet_dataset_sample_10K.tsv\", encoding=\"utf-8\", sep=\",\")\n",
        "\n",
        "# Remove the space in the first, second\n",
        "df[\"first\"] = df[\"first\"].apply(lambda x: x.replace(\" \", \"\"))\n",
        "df[\"second\"] = df[\"second\"].apply(lambda x: x.replace(\" \", \"\"))\n",
        "\n",
        "print(\"Length before: \" + str(len(df)))\n",
        "df = df.groupby('label').filter(lambda x: len(x) > 1)\n",
        "print(\"Length after: \" + str(len(df)))\n",
        "\n",
        "# # Combine the first and second columns into a new column text\n",
        "df[\"text\"] = df[\"first\"] + '\\n' + df[\"second\"]\n",
        "\n",
        "# # Remove the first, second columns\n",
        "df = df.drop(columns=[\"first\", \"second\"])\n",
        "\n",
        "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "\n",
        "with open(f\"{DATA}/split/train.txt\", \"w\") as f:\n",
        "    for text in df_train[\"text\"]:\n",
        "        f.write(text + \"\\n\")\n",
        "\n",
        "with open(f\"{DATA}/split/valid.txt\", \"w\") as f:\n",
        "    for text in df_val[\"text\"]:\n",
        "        f.write(text + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcHShB4Asdjs"
      },
      "source": [
        "## Read Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0cfYEFzsdjs"
      },
      "outputs": [],
      "source": [
        "# META = pd.read_csv(DATA/\"meta.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIixirHhsdjs"
      },
      "outputs": [],
      "source": [
        "LABELS = ['train.txt', 'valid.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnvKgkzysdjs"
      },
      "outputs": [],
      "source": [
        "punkt_regex = r'[^\\w\\s]'\n",
        "\n",
        "def position_of_all_punctuation(x):\n",
        "    return [m.start() for m in re.finditer(punkt_regex, x)]\n",
        "\n",
        "# simplify the punctuation\n",
        "eng_punkt_to_cn_dict = {\n",
        "    \".\": \"。\",\n",
        "    \",\": \"，\",\n",
        "    \":\": \"：\",\n",
        "    \";\": \"；\",\n",
        "    \"?\": \"？\",\n",
        "    \"!\": \"！\",\n",
        "    \"“\": \"\\\"\",\n",
        "    \"”\": \"\\\"\",\n",
        "    \"‘\": \"\\'\",\n",
        "    \"’\": \"\\'\",\n",
        "    \"「\": \"（\",\n",
        "    \"」\": \"）\",\n",
        "    \"『\": \"\\\"\",\n",
        "    \"』\": \"\\\"\",\n",
        "    \"（\": \"（\",\n",
        "    \"）\": \"）\",\n",
        "    \"《\": \"【\",\n",
        "    \"》\": \"】\",\n",
        "    \"［\": \"【\",\n",
        "    \"］\": \"】\",\n",
        "    }\n",
        "\n",
        "def translate_eng_punkt_to_cn(char):\n",
        "    if char == \"0\":\n",
        "        return char\n",
        "    if char in eng_punkt_to_cn_dict.values():\n",
        "        return char\n",
        "    result = eng_punkt_to_cn_dict.get(char)\n",
        "    if result is None:\n",
        "        return \"。\"\n",
        "    return result\n",
        "\n",
        "def punct_ner_pair(sentence):\n",
        "    positions = position_of_all_punctuation(sentence)\n",
        "    x = re.sub(punkt_regex, '', sentence)\n",
        "    y = list(\"0\"*len(x))\n",
        "\n",
        "    for i, p in enumerate(positions):\n",
        "        y[p-i-1] = sentence[p]\n",
        "    p_df = pd.DataFrame({\"x\":list(x), \"y\":y})\n",
        "    p_df[\"y\"] = p_df[\"y\"].apply(translate_eng_punkt_to_cn)\n",
        "    return p_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rLuqkoJsdjt"
      },
      "outputs": [],
      "source": [
        "ALL_LABELS = [\"0\",] +list(eng_punkt_to_cn_dict.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnxEnvuFqOXF"
      },
      "outputs": [],
      "source": [
        "print(ALL_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igm2oCW6sdjt"
      },
      "outputs": [],
      "source": [
        "cates = Category(ALL_LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "_yR7bwAbsdjt"
      },
      "outputs": [],
      "source": [
        "class PunctDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: Path,\n",
        "        filelist: List[str],\n",
        "        num_threads: int = 8,\n",
        "        length: int = 1000,\n",
        "        size: int = 540\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            - filelist: list of file names\n",
        "            - The dataset will open ```num_threads``` files, and hold\n",
        "                in memory simoultaneously.\n",
        "            - num_threads: number of threads to read files,\n",
        "            - length: number of sentences per batch\n",
        "            - size: number of characters per sentence\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.filelist = filelist\n",
        "        self.num_threads = num_threads\n",
        "        self.length = length\n",
        "        # open file strings, index is mod of num_threads\n",
        "        self.current_files = dict(enumerate([\"\"]*length))\n",
        "        self.string_index = dict(enumerate([0]*length))\n",
        "        self.to_open_idx = 0\n",
        "        self.size = size\n",
        "        self.get_counter = 0\n",
        "        self.return_string = False\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"PunctDataset: {len(self)}, on {len(self.filelist)} files\"\n",
        "\n",
        "    def new_file(self, idx_mod):\n",
        "        filename = self.filelist[self.to_open_idx]\n",
        "        with open(self.data_dir/filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.current_files[idx_mod] = f.read()\n",
        "\n",
        "        self.to_open_idx += 1\n",
        "\n",
        "        # reset to open article file index\n",
        "        if self.to_open_idx >= len(self.filelist):\n",
        "            self.to_open_idx = 0\n",
        "\n",
        "        # reset string_index within new article file\n",
        "        self.string_index[idx_mod] = 0\n",
        "\n",
        "        # if self.to_open_idx % 500 == 0:\n",
        "        #     print(f\"went through files:\\t{self.to_open_idx}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx_mod = self.get_counter % self. num_threads\n",
        "\n",
        "        if self.string_index[idx_mod] >= len(self.current_files[idx_mod]):\n",
        "            self.new_file(idx_mod)\n",
        "        string_idx = self.string_index[idx_mod]\n",
        "\n",
        "        # slicing a sentence\n",
        "        sentence = self.current_files[idx_mod][string_idx:string_idx+self.size]\n",
        "\n",
        "        # move the string_index within current article file\n",
        "        self.string_index[idx_mod] += self.size\n",
        "\n",
        "        # move the get_counter\n",
        "        self.get_counter += 1\n",
        "        p_df = punct_ner_pair(sentence)\n",
        "        return list(p_df.x), list(p_df.y)\n",
        "\n",
        "    def align_offsets(\n",
        "        self,\n",
        "        inputs,\n",
        "        text_labels: List[List[str]],\n",
        "        words: List[List[str]]\n",
        "    ):\n",
        "        \"\"\"\n",
        "        inputs: output if tokenizer\n",
        "        text_labels: labels in form of list of list of strings\n",
        "        words: words in form of list of list of strings\n",
        "        \"\"\"\n",
        "        labels = torch.zeros_like(inputs.input_ids).long()\n",
        "        labels -= 100\n",
        "        text_lables_array = np.empty(labels.shape, dtype=object)\n",
        "        words_array = np.empty(labels.shape, dtype=object)\n",
        "        max_len = inputs.input_ids.shape[1]\n",
        "\n",
        "        # print(\"Input_ids: \", inputs.input_ids)\n",
        "\n",
        "        # print(\"Max Len: \", max_len)\n",
        "\n",
        "        # print(\"Text Labels: \", text_labels)\n",
        "\n",
        "        for row_id, input_ids in enumerate(inputs.input_ids):\n",
        "            word_pos = inputs.word_ids(row_id)\n",
        "            # print(\"Word Pos: \", word_pos)\n",
        "            for idx, pos in enumerate(word_pos):\n",
        "                # print(\"index: \", idx)\n",
        "                if pos is None:\n",
        "                    # print(\"Pos is None\")\n",
        "                    continue\n",
        "                labels[row_id, idx] = self.cates.c2i[text_labels[row_id][pos]]\n",
        "                if self.return_string:\n",
        "                    text_lables_array[row_id,\n",
        "                                        idx] = text_labels[row_id][pos]\n",
        "                    words_array[row_id, idx] = words[row_id][pos]\n",
        "\n",
        "        inputs['labels'] = labels\n",
        "\n",
        "        if self.return_string:\n",
        "            inputs['text_labels'] = text_lables_array.tolist()\n",
        "            inputs['word'] = words_array.tolist()\n",
        "\n",
        "        # for input_id in inputs['input_ids']:\n",
        "        #     print(\"InPuT_iD: \", input_id)\n",
        "        #     print(\"Word: \", self.tokenizer.convert_ids_to_tokens(input_id))\n",
        "        #     print(\"Word Pos: \", inputs.word_ids(0))\n",
        "        # for label in inputs['labels']:\n",
        "        #     print(\"Label: \", label)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        \"\"\"\n",
        "        data: list of tuple\n",
        "        \"\"\"\n",
        "        words, text_labels = zip(*data)\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            list(words),\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            is_split_into_words=True,\n",
        "            return_offsets_mapping=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "\n",
        "        # print(\"Original words\", words)\n",
        "\n",
        "        return self.align_offsets(inputs, text_labels, words)\n",
        "\n",
        "    def dataloaders(self, tokenizer, cates, max_len: int = 512, batch_size: int = 32):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.cates = cates\n",
        "        self.max_len = max_len\n",
        "        return DataLoader(\n",
        "            self,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=self.collate_fn,\n",
        "        )\n",
        "\n",
        "    def split(self, ratio: float = 0.9, n_splits: int = 5):\n",
        "      \"\"\"\n",
        "      Split the dataset into train and valid using StratifiedKFold\n",
        "      \"\"\"\n",
        "\n",
        "      train_filelist = [\"train.txt\"]\n",
        "      valid_filelist = [\"valid.txt\"]\n",
        "\n",
        "      train_dataset = PunctDataset(\n",
        "          self.data_dir,\n",
        "          train_filelist,\n",
        "          num_threads=self.num_threads,\n",
        "          length=int(self.length * ratio),\n",
        "          size=self.size,\n",
        "      )\n",
        "      valid_dataset = PunctDataset(\n",
        "          self.data_dir,\n",
        "          valid_filelist,\n",
        "          num_threads=self.num_threads,\n",
        "          length=int(self.length * (1 - ratio)),\n",
        "          size=self.size,\n",
        "      )\n",
        "      return train_dataset, valid_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkKOiGZgsdju"
      },
      "source": [
        "Create dataset object\n",
        "\n",
        "* Length is the length of the epoch\n",
        "* Size: is the sequence length\n",
        "* num_threads: num of files that is opening at the same time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j30XzwXRsdju"
      },
      "outputs": [],
      "source": [
        "ds = PunctDataset(DATA + '/split', LABELS, num_threads=1, length=19566, size=512)\n",
        "train_ds, valid_ds = ds.split(0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OsERZ2Msdju"
      },
      "source": [
        "### lightning data module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCAlLXadsdju"
      },
      "outputs": [],
      "source": [
        "class PunctDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_ds, valid_ds, tokenizer, cates,\n",
        "    max_len=512, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.train_ds, self.valid_ds = train_ds, valid_ds\n",
        "        self.tokenizer = tokenizer\n",
        "        self.cates = cates\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def split_data(self):\n",
        "\n",
        "        return train_ds, valid_ds\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.train_ds.dataloaders(\n",
        "            self.tokenizer,\n",
        "            self.cates,\n",
        "            self.max_len,\n",
        "            self.batch_size,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.valid_ds.dataloaders(\n",
        "            self.tokenizer,\n",
        "            self.cates,\n",
        "            self.max_len,\n",
        "            self.batch_size*4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQwWK9Vzsdju"
      },
      "source": [
        "## Load Pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naBrxECesdju"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"raynardj/classical-chinese-punctuation-guwen-biaodian\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv9N_Ywzsdju"
      },
      "outputs": [],
      "source": [
        "from forgebox.thunder.callbacks import DataFrameMetricsCallback\n",
        "from forgebox.hf.train import NERModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpQCUifXsdju"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define BERT-CRF Model using PyTorch Lightning\n",
        "class BERT_CRF(pl.LightningModule):\n",
        "    def __init__(self, model, num_labels, learning_rate=1e-5):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=['model'])\n",
        "\n",
        "        self.model = model\n",
        "        self.crf = CRF(num_labels, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.model.config.hidden_size, num_labels)\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        sequence_output = output.last_hidden_state\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.fc(sequence_output)\n",
        "        if labels is not None:\n",
        "            loss = -self.crf(logits, labels, mask=attention_mask.bool(), reduction='mean')\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        else:\n",
        "            return {\"logits\": logits}\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, logits = self(input_ids, attention_mask, labels)\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', self.crf.accuracy(logits, labels, attention_mask.byte()))\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, logits = self(input_ids, attention_mask, labels)\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_acc', self.crf.accuracy(logits, labels, attention_mask.byte()))\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SROrDVf5sdjv"
      },
      "source": [
        "Load pretrained model with proper num of categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyjCirgLsdjv"
      },
      "outputs": [],
      "source": [
        "model = BertForTokenClassification.from_pretrained(\"raynardj/classical-chinese-punctuation-guwen-biaodian\", num_labels=len(cates),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf15wg66sdjv"
      },
      "outputs": [],
      "source": [
        "print(train_ds.length)\n",
        "print(valid_ds.length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LroOwXvEsdjv"
      },
      "outputs": [],
      "source": [
        "data_module = PunctDataModule(train_ds, valid_ds, tokenizer, cates,\n",
        "                              batch_size=32,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4qvb_1Lsdjv"
      },
      "source": [
        "### Run data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILg2XCnAsdjv"
      },
      "outputs": [],
      "source": [
        "inputs = next(iter(data_module.val_dataloader()))\n",
        "print(inputs.input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUWK6mJksdjv"
      },
      "outputs": [],
      "source": [
        "inputs.input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wctE5zlAsdjv"
      },
      "outputs": [],
      "source": [
        "inputs.labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDBPGQi9sdjv"
      },
      "source": [
        "## NER tranining module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI-Bbrg8sdjv"
      },
      "outputs": [],
      "source": [
        "module = NERModule(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzOnN4e-sdjv"
      },
      "outputs": [],
      "source": [
        "save_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath=f\"/content/gdrive/MyDrive/Colab/punktuation-ner/ckpoint\",\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        ")\n",
        "df_show = DataFrameMetricsCallback()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF1Wy_dysdjv"
      },
      "source": [
        "Reset the configure_optimizers function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7_Lg0Uasdjv"
      },
      "outputs": [],
      "source": [
        "def configure_optimizers(self):\n",
        "        # discriminative learning rate\n",
        "    param_groups = [\n",
        "            {'params': self.model.bert.parameters(), 'lr': 5e-6},\n",
        "            {'params': self.model.classifier.parameters(), 'lr': 1e-3},\n",
        "        ]\n",
        "    optimizer = torch.optim.Adam(param_groups, lr=1e-3)\n",
        "    return optimizer\n",
        "\n",
        "NERModule.configure_optimizers = configure_optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBa18c_Hsdjw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrRWPrHSsdjw"
      },
      "source": [
        "Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7S2ZyqJsdjw"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    accelerator='gpu',\n",
        "    devices=1,\n",
        "    max_epochs=30,\n",
        "    callbacks=[df_show, save_callback],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ9HK7-osdjw"
      },
      "outputs": [],
      "source": [
        "trainer.fit(module, datamodule=data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4D61WzHsdjw"
      },
      "source": [
        "## Load the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jNl0oe8sdjw"
      },
      "outputs": [],
      "source": [
        "module = NERModule.load_from_checkpoint(save_callback.best_model_path, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mv3Xb1Usdjw"
      },
      "outputs": [],
      "source": [
        "module.model.config.id2label = dict(enumerate(cates.i2c))\n",
        "module.model.config.label2id = cates.c2i.dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p9Qis9isdj1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgmMa7TDsdj1"
      },
      "outputs": [],
      "source": [
        "module.model = module.model.eval()\n",
        "module.model = module.model.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftvX_cZswbBh"
      },
      "outputs": [],
      "source": [
        "# prompt: Store the model\n",
        "\n",
        "torch.save(module.model, '/content/gdrive/MyDrive/Colab/punktuation-ner/best_ckpoint/punct_model_2.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCzuBXI_sdj1"
      },
      "outputs": [],
      "source": [
        "ner = pipeline(\"ner\",module.model,tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO9PhdmLsdj1"
      },
      "outputs": [],
      "source": [
        "def mark_sentence(x: str):\n",
        "    outputs = ner(x)\n",
        "    print(outputs)\n",
        "    print(\"hello\")\n",
        "    x_list = list(x)\n",
        "    for i, output in enumerate(outputs):\n",
        "        x_list.insert(output['end']+i, output['entity'])\n",
        "    return \"\".join(x_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEtCNB18sdj1"
      },
      "outputs": [],
      "source": [
        "mark_sentence(\"前向香江壹帶澄清來活水\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGGOgoY9sdj1"
      },
      "outputs": [],
      "source": [
        "mark_sentence(\"\"\"郡邑置夫子庙于学以嵗时释奠盖自唐贞观以来未之或改我宋有天下因其制而损益之姑苏当浙右要区规模尤大更建炎戎马荡然无遗虽修学宫于荆榛瓦砾之余独殿宇未遑议也每春秋展礼于斋庐已则置不问殆为阙典今寳文阁直学士括苍梁公来牧之明年实绍兴十有一禩也二月上丁修祀既毕乃愓然自咎揖诸生而告之曰天子不以汝嘉为不肖俾再守兹土顾治民事神皆守之职惟是夫子之祀教化所基尤宜严且谨而拜跪荐祭之地卑陋乃尔其何以掲防妥灵汝嘉不敢避其责曩常去此弥年若有所负尚安得以罢輭自恕复累后人乎他日或克就绪愿与诸君落之于是谋之僚吏搜故府得遗材千枚取赢资以给其费鸠工庀役各举其任嵗月讫工民不与知像设礼器百用具修至于堂室廊序门牖垣墙皆一新之\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0krTpEd3sdj2"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# df = pd.read_csv('test.txt', sep='\\t')\n",
        "\n",
        "# text_list = df['input'].tolist()[0:]\n",
        "# predicted = []\n",
        "# for text in text_list:\n",
        "#     predicted.append(mark_sentence(text))\n",
        "\n",
        "# # predicted save to csv\n",
        "# df2 = pd.DataFrame(predicted)\n",
        "# # output to csv but ignore header\n",
        "# df2.to_csv('predicted.txt', index=False, header=False)\n",
        "# df2 = pd.read_csv('predicted.txt', sep='\\t', header=None)\n",
        "# df2.columns = ['predicted']\n",
        "\n",
        "# punctuation = ['，', '。', '！', '？', '；', '：', '、', '「', '」', '『', '』', '（', '）', '〔', '〕', '【', '】', '《', '》', '〈', '〉', '﹏', '＿', '～', '—', '…', '‥', '﹑', '﹔', '﹖', '﹪', '﹙', '﹚', '﹛', '﹜', '﹟', '﹠', '﹡', '﹢', '﹣', '﹤', '﹥', '﹦', '﹨', '﹩', '﹪', '﹫', '＃', '＄', '％', '＆', '＊', '＋', '－', '／', '＜', '＝', '＞', '＠', '＾', '＿', '｀', '｜', '～', '∕', '∥']\n",
        "\n",
        "# predicted_parsed = []\n",
        "# for text in df2['predicted']:\n",
        "#     i = 0\n",
        "#     line = []\n",
        "#     while i < len(text) - 1:\n",
        "#         if text[i + 1] in punctuation:\n",
        "#             line.append(1)\n",
        "#             i += 1\n",
        "#         else:\n",
        "#             line.append(0)\n",
        "#         i += 1\n",
        "#     if line[-1] != 1:\n",
        "#         line.append(1)\n",
        "#     predicted_parsed.append(' - '.join([str(x) for x in line]))\n",
        "\n",
        "# df3 = pd.DataFrame(predicted_parsed)\n",
        "# df3.to_csv('predicted_parsed.txt', index=False, header=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}