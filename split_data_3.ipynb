{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_for_split(file_path):\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            text = line.replace(' ', '')\n",
    "            label = []\n",
    "            i = 0\n",
    "            while (i < len(text)):\n",
    "                if text[i] in ['，', '。', '？', '！']:\n",
    "                    label.append('1')\n",
    "                else:\n",
    "                    label.append('0')\n",
    "                i += 1\n",
    "            text_list = list(text)\n",
    "            # label[-1] = 2\n",
    "            # print(text, label)\n",
    "            # break\n",
    "            texts.append(text_list)\n",
    "            labels.append(''.join(label))\n",
    "\n",
    "            if (len(text_list) != len(label)):\n",
    "                print('Error:', text, label)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = load_data_for_split('data/train_large_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['忽', '忽', '幾', '晨', '昏', '，', '離', '別', '間', '之', '，', '疾', '病', '間', '之', '，', '不', '及', '終', '年', '同', '靜', '好', '。'], ['煢', '煢', '小', '兒', '女', '，', '孱', '羸', '若', '此', '，', '嬌', '憨', '若', '此', '，', '更', '煩', '二', '老', '費', '精', '神', '。'], ['毋', '人', '負', '我', '，', '毋', '我', '負', '人', '，', '柳', '下', '雖', '和', '有', '介', '稱', '，', '先', '生', '字', '此', '，', '可', '以', '諡', '此', '。'], ['愛', '老', '臣', '少', '，', '愛', '少', '臣', '老', '，', '馮', '唐', '爲', '郎', '無', '倦', '意', '，', '吾', '輩', '慕', '之', '，', '不', '能', '效', '之', '。'], ['深', '院', '落', '滕', '花', '，', '石', '不', '點', '頭', '龍', '不', '語', '。']]\n",
      "['000001000010000100000001', '000001000010000100000001', '0000100001000000010000100001', '0000100001000000010000100001', '00000100000001']\n"
     ]
    }
   ],
   "source": [
    "print(texts[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000001000010000100000001', '000001000010000100000001', '0000100001000000010000100001', '0000100001000000010000100001', '00000100000001']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the ratio of punctuation marks\n",
    "bins = labels\n",
    "print(bins[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "for i in range(len(labels)):\n",
    "    groups.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame({'labels': labels, 'bins': bins, 'texts': texts, 'groups': groups})\n",
    "\n",
    "# Step 3: Stratified K-Fold Cross Validation using the binned categories\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "sgkf = StratifiedGroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output df to csv\n",
    "df.to_csv('data/train_large_bin_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nhat Hung\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\model_selection\\_split.py:885: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368352 592088\n"
     ]
    }
   ],
   "source": [
    "cnt_train = 0\n",
    "cnt_valid = 0\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(sgkf.split(df['texts'], df['bins'], df['groups'])):\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[val_index]\n",
    "    \n",
    "    X_train, y_train = train_df['texts'].tolist(), train_df['bins']\n",
    "    X_val, y_val = val_df['texts'].tolist(), val_df['bins']\n",
    "\n",
    "    \n",
    "    with open(f'data/split3/train_{i}.txt', 'w', encoding='utf-8') as file:\n",
    "        for text, label in zip(X_train, y_train):\n",
    "            file.write(''.join(text) + '\\n')\n",
    "            cnt_train += 1\n",
    "\n",
    "    with open(f'data/split3/valid_{i}.txt', 'w', encoding='utf-8') as file:\n",
    "        for text, label in zip(X_val, y_val):\n",
    "            file.write(''.join(text) + '\\n')\n",
    "            cnt_valid += 1\n",
    "\n",
    "print(cnt_train, cnt_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nhat Hung\\AppData\\Local\\Programs\\Orange\\lib\\site-packages\\sklearn\\model_selection\\_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368352 592088\n"
     ]
    }
   ],
   "source": [
    "cnt_train = 0\n",
    "cnt_valid = 0\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(df['texts'], df['bins'])):\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[val_index]\n",
    "    \n",
    "    X_train, y_train = train_df['texts'].tolist(), train_df['bins']\n",
    "    X_val, y_val = val_df['texts'].tolist(), val_df['bins']\n",
    "\n",
    "    \n",
    "    with open(f'data/split3/train_{i}.txt', 'w', encoding='utf-8') as file:\n",
    "        for text, label in zip(X_train, y_train):\n",
    "            file.write(''.join(text) + '\\n')\n",
    "            cnt_train += 1\n",
    "\n",
    "    with open(f'data/split3/valid_{i}.txt', 'w', encoding='utf-8') as file:\n",
    "        for text, label in zip(X_val, y_val):\n",
    "            file.write(''.join(text) + '\\n')\n",
    "            cnt_valid += 1\n",
    "\n",
    "print(cnt_train, cnt_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Assuming your data is in a pandas DataFrame with columns 'sentence' and 'label'\n",
    "# If your data is in another format, adapt this part accordingly\n",
    "# For example:\n",
    "# data = pd.DataFrame({'sentence': sentences, 'label': labels})\n",
    "\n",
    "# Load your data (replace with your actual loading mechanism)\n",
    "sentences, labels = load_data_for_split('data/train_large_2.txt')\n",
    "data = pd.DataFrame({'sentence': sentences, 'label': labels})\n",
    "# Initialize StratifiedKFold\n",
    "n_splits = 64  # We want to split into 10 folds\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Split the data into 10 stratified folds\n",
    "for i, (train_index, test_index) in enumerate(skf.split(sentences, labels)):\n",
    "    # Extract the corresponding fold\n",
    "    fold_sentences = [''.join(sentences[idx]) for idx in test_index]\n",
    "    \n",
    "    # Write the fold to a file\n",
    "    fold_df = pd.DataFrame({'sentence': fold_sentences})  \n",
    "    fold_df.to_csv(f'data/split4/valid_{i+1}.csv', index=False, header=False)  \n",
    "    print(f'Valid {i+1} saved with {len(fold_sentences)} sentences.')\n",
    "\n",
    "    fold_sentences = [''.join(sentences[idx]) for idx in train_index]\n",
    "    fold_df = pd.DataFrame({'sentence': fold_sentences})\n",
    "    fold_df.to_csv(f'data/split4/train_{i+1}.csv', index=False, header=False)\n",
    "    print(f'Train {i+1} saved with {len(fold_sentences)} sentences.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def recursive_split(data, n_splits, idx=0):\n",
    "    print(f'Splitting data_{idx} with {len(data)} samples')\n",
    "    if idx >= n_splits:\n",
    "        with open(f'data/split5/data_{idx - n_splits}.txt', 'w', encoding='utf-8') as file:\n",
    "            for text in data['sentence'].to_list():\n",
    "                file.write(''.join(text) + '\\n')\n",
    "        return\n",
    "    train_data, test_data = train_test_split(data, test_size=0.5, random_state=42, stratify=data['label'])\n",
    "    # print(f'Lenght of train_data: {len(train_data)}')\n",
    "    # print(f'Lenght of test_data: {len(test_data)}')\n",
    "    recursive_split(train_data, n_splits, idx * 2)\n",
    "    recursive_split(test_data, n_splits, idx * 2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = load_data_for_split('data/train_large_2.txt')\n",
    "\n",
    "data = pd.DataFrame({'sentence': sentences, 'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = data['label'].value_counts()\n",
    "valid_labels = label_counts[label_counts >= 64].index\n",
    "filtered_data = data[data['label'].isin(valid_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data with 592088 samples\n",
      "Filtered data with 569625 samples\n"
     ]
    }
   ],
   "source": [
    "print(f'Original data with {len(data)} samples')\n",
    "print(f'Filtered data with {len(filtered_data)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data_1 with 569625 samples\n",
      "Splitting data_2 with 284812 samples\n",
      "Splitting data_4 with 142406 samples\n",
      "Splitting data_8 with 71203 samples\n",
      "Splitting data_16 with 35601 samples\n",
      "Splitting data_32 with 17800 samples\n",
      "Splitting data_64 with 8900 samples\n",
      "Splitting data_65 with 8900 samples\n",
      "Splitting data_33 with 17801 samples\n",
      "Splitting data_66 with 8900 samples\n",
      "Splitting data_67 with 8901 samples\n",
      "Splitting data_17 with 35602 samples\n",
      "Splitting data_34 with 17801 samples\n",
      "Splitting data_68 with 8900 samples\n",
      "Splitting data_69 with 8901 samples\n",
      "Splitting data_35 with 17801 samples\n",
      "Splitting data_70 with 8900 samples\n",
      "Splitting data_71 with 8901 samples\n",
      "Splitting data_9 with 71203 samples\n",
      "Splitting data_18 with 35601 samples\n",
      "Splitting data_36 with 17800 samples\n",
      "Splitting data_72 with 8900 samples\n",
      "Splitting data_73 with 8900 samples\n",
      "Splitting data_37 with 17801 samples\n",
      "Splitting data_74 with 8900 samples\n",
      "Splitting data_75 with 8901 samples\n",
      "Splitting data_19 with 35602 samples\n",
      "Splitting data_38 with 17801 samples\n",
      "Splitting data_76 with 8900 samples\n",
      "Splitting data_77 with 8901 samples\n",
      "Splitting data_39 with 17801 samples\n",
      "Splitting data_78 with 8900 samples\n",
      "Splitting data_79 with 8901 samples\n",
      "Splitting data_5 with 142406 samples\n",
      "Splitting data_10 with 71203 samples\n",
      "Splitting data_20 with 35601 samples\n",
      "Splitting data_40 with 17800 samples\n",
      "Splitting data_80 with 8900 samples\n",
      "Splitting data_81 with 8900 samples\n",
      "Splitting data_41 with 17801 samples\n",
      "Splitting data_82 with 8900 samples\n",
      "Splitting data_83 with 8901 samples\n",
      "Splitting data_21 with 35602 samples\n",
      "Splitting data_42 with 17801 samples\n",
      "Splitting data_84 with 8900 samples\n",
      "Splitting data_85 with 8901 samples\n",
      "Splitting data_43 with 17801 samples\n",
      "Splitting data_86 with 8900 samples\n",
      "Splitting data_87 with 8901 samples\n",
      "Splitting data_11 with 71203 samples\n",
      "Splitting data_22 with 35601 samples\n",
      "Splitting data_44 with 17800 samples\n",
      "Splitting data_88 with 8900 samples\n",
      "Splitting data_89 with 8900 samples\n",
      "Splitting data_45 with 17801 samples\n",
      "Splitting data_90 with 8900 samples\n",
      "Splitting data_91 with 8901 samples\n",
      "Splitting data_23 with 35602 samples\n",
      "Splitting data_46 with 17801 samples\n",
      "Splitting data_92 with 8900 samples\n",
      "Splitting data_93 with 8901 samples\n",
      "Splitting data_47 with 17801 samples\n",
      "Splitting data_94 with 8900 samples\n",
      "Splitting data_95 with 8901 samples\n",
      "Splitting data_3 with 284813 samples\n",
      "Splitting data_6 with 142406 samples\n",
      "Splitting data_12 with 71203 samples\n",
      "Splitting data_24 with 35601 samples\n",
      "Splitting data_48 with 17800 samples\n",
      "Splitting data_96 with 8900 samples\n",
      "Splitting data_97 with 8900 samples\n",
      "Splitting data_49 with 17801 samples\n",
      "Splitting data_98 with 8900 samples\n",
      "Splitting data_99 with 8901 samples\n",
      "Splitting data_25 with 35602 samples\n",
      "Splitting data_50 with 17801 samples\n",
      "Splitting data_100 with 8900 samples\n",
      "Splitting data_101 with 8901 samples\n",
      "Splitting data_51 with 17801 samples\n",
      "Splitting data_102 with 8900 samples\n",
      "Splitting data_103 with 8901 samples\n",
      "Splitting data_13 with 71203 samples\n",
      "Splitting data_26 with 35601 samples\n",
      "Splitting data_52 with 17800 samples\n",
      "Splitting data_104 with 8900 samples\n",
      "Splitting data_105 with 8900 samples\n",
      "Splitting data_53 with 17801 samples\n",
      "Splitting data_106 with 8900 samples\n",
      "Splitting data_107 with 8901 samples\n",
      "Splitting data_27 with 35602 samples\n",
      "Splitting data_54 with 17801 samples\n",
      "Splitting data_108 with 8900 samples\n",
      "Splitting data_109 with 8901 samples\n",
      "Splitting data_55 with 17801 samples\n",
      "Splitting data_110 with 8900 samples\n",
      "Splitting data_111 with 8901 samples\n",
      "Splitting data_7 with 142407 samples\n",
      "Splitting data_14 with 71203 samples\n",
      "Splitting data_28 with 35601 samples\n",
      "Splitting data_56 with 17800 samples\n",
      "Splitting data_112 with 8900 samples\n",
      "Splitting data_113 with 8900 samples\n",
      "Splitting data_57 with 17801 samples\n",
      "Splitting data_114 with 8900 samples\n",
      "Splitting data_115 with 8901 samples\n",
      "Splitting data_29 with 35602 samples\n",
      "Splitting data_58 with 17801 samples\n",
      "Splitting data_116 with 8900 samples\n",
      "Splitting data_117 with 8901 samples\n",
      "Splitting data_59 with 17801 samples\n",
      "Splitting data_118 with 8900 samples\n",
      "Splitting data_119 with 8901 samples\n",
      "Splitting data_15 with 71204 samples\n",
      "Splitting data_30 with 35602 samples\n",
      "Splitting data_60 with 17801 samples\n",
      "Splitting data_120 with 8900 samples\n",
      "Splitting data_121 with 8901 samples\n",
      "Splitting data_61 with 17801 samples\n",
      "Splitting data_122 with 8900 samples\n",
      "Splitting data_123 with 8901 samples\n",
      "Splitting data_31 with 35602 samples\n",
      "Splitting data_62 with 17801 samples\n",
      "Splitting data_124 with 8900 samples\n",
      "Splitting data_125 with 8901 samples\n",
      "Splitting data_63 with 17801 samples\n",
      "Splitting data_126 with 8900 samples\n",
      "Splitting data_127 with 8901 samples\n"
     ]
    }
   ],
   "source": [
    "recursive_split(filtered_data, 64, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
